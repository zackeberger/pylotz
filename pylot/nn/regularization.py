import torch
from torch import nn
import torch.autograd


# Functions to perform L1 and L2 regularization on the output of a layer
# Specially useful to simulate L1 and L2 regularization of a virtual primary
# network case whose weights are generated by a hypernetwork

# Note: the implementation does not modify the loss function, instead
# it's implemented by a autograd function that modifies the gradients
# to account for weight decay


class L2OutputRegularizationFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input, weight_decay):
        ctx.save_for_backward(input, weight_decay)
        return input

    @staticmethod
    def backward(ctx, grad_output):
        output, weight_decay = ctx.saved_tensors
        grad_output = grad_output.add(output, alpha=weight_decay)
        return grad_output, None


class L2ActivationRegularizer(nn.Module):
    def __init__(self, weight_decay=1e-4):
        super().__init__()
        self.register_buffer("weight_decay", torch.tensor(weight_decay))

    def forward(self, input):
        return L2OutputRegularizationFunction.apply(input, self.weight_decay)


class L1OutputRegularizationFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input, weight_decay):
        ctx.save_for_backward(torch.sign(input), weight_decay)
        return input

    @staticmethod
    def backward(ctx, grad_output):
        output_sign, weight_decay = ctx.saved_tensors
        grad_output = grad_output.add(output_sign, alpha=weight_decay)
        return grad_output, None


class L1ActivationRegularizer(nn.Module):
    def __init__(self, weight_decay=1e-4):
        super().__init__()
        self.register_buffer("weight_decay", torch.tensor(weight_decay))

    def forward(self, input):
        return L1OutputRegularizationFunction.apply(input, self.weight_decay)
